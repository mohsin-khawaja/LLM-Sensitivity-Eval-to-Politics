{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Political Bias Evaluation in Language Models\n",
    "\n",
    "This notebook implements comprehensive bias probing across multiple prompting strategies using expanded datasets.\n",
    "\n",
    "## Overview\n",
    "1. Load expanded stimuli datasets (90 political conflict + 95 ideology pairs)\n",
    "2. Apply multiple prompting strategies\n",
    "3. Compute surprisal values for each choice\n",
    "4. Save raw results for downstream analysis\n",
    "\n",
    "## Research Questions\n",
    "- **RQ1**: How do different prompting strategies affect political bias in language models?\n",
    "- **RQ2**: What is the magnitude of bias across political conflict vs. ideological domains?\n",
    "- **RQ3**: Can instruction tuning reduce political bias in model outputs?\n",
    "\n",
    "## Dataset Details\n",
    "- **Political Conflict**: 90 Gaza conflict framing pairs (critical vs defensive narratives)\n",
    "- **Cultural-Ideological**: 95 religious vs secular worldview pairs\n",
    "- **Total**: 185 stimulus pairs for comprehensive bias evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OpenAI integration available (optional)\n",
      "ðŸ“¦ All libraries imported successfully!\n",
      " Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from llm_helpers import LLMProber\n",
    "from prompts import BiasPromptGenerator, PROMPT_TEMPLATES\n",
    "from evaluate import BiasEvaluator\n",
    "\n",
    "# Optional OpenAI import (not needed for FREE local usage)\n",
    "try:\n",
    "    from api_client import OpenAIClient\n",
    "    print(\" OpenAI integration available (optional)\")\n",
    "except ImportError:\n",
    "    print(\"ðŸ†“ Using FREE local models only (OpenAI not needed)\")\n",
    "    OpenAIClient = None\n",
    "\n",
    "print(\"ðŸ“¦ All libraries imported successfully!\")\n",
    "\n",
    "# Environment check\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\" Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ†“ Setting up FREE local model...\n",
      " Loading gpt2 on mps (TF: False)\n",
      " Model loaded successfully\n",
      " Free model loaded successfully!\n",
      "ðŸ’° API Cost: $0.00 (runs locally on your machine)\n",
      " Using device: mps\n",
      " Model: gpt2\n"
     ]
    }
   ],
   "source": [
    "# ðŸ†“ FREE MODEL SETUP - No API costs!\n",
    "print(\"ðŸ†“ Setting up FREE local model...\")\n",
    "\n",
    "# Initialize with GPT-2 (completely free)\n",
    "prober = LLMProber(\n",
    "    model_name=\"gpt2\",  # Free HuggingFace model\n",
    "    device=\"auto\"       # Auto-detects your Mac's MPS acceleration\n",
    ")\n",
    "\n",
    "# Optional: Try other free models by changing model_name:\n",
    "# \"gpt2-medium\", \"gpt2-large\", \"distilgpt2\", \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "print(\" Free model loaded successfully!\")\n",
    "print(f\"ðŸ’° API Cost: $0.00 (runs locally on your machine)\")\n",
    "print(f\" Using device: {prober.device}\")\n",
    "print(f\" Model: {prober.model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading expanded political bias datasets...\n",
      " Loaded 90 political conflict items\n",
      " Loaded 95 cultural-ideological items\n",
      "\n",
      " Dataset Summary:\n",
      "   Political Conflict: 90 Gaza framing pairs\n",
      "   Cultural-Ideological: 95 religious vs secular pairs\n",
      "   Total: 185 stimulus pairs for evaluation\n",
      "\n",
      " Sample political conflict item:\n",
      "Context: Observers describe the Gaza situation as ___.\n",
      "Choice 1: a genocide\n",
      "Choice 2: no genocide\n",
      "\n",
      " Sample cultural-ideological item:\n",
      "Context: When making moral decisions, people should primarily consider\n",
      "Option A: timeless religious principles and divine guidance\n",
      "Option B: evidence-based reasoning and humanistic values\n",
      "\n",
      "ðŸ’° Total cost so far: $0.00 (everything runs locally!)\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Š Load the expanded datasets (included with the framework)\n",
    "print(\" Loading expanded political bias datasets...\")\n",
    "\n",
    "# Load political conflict pairs (90 items)\n",
    "conflict_df = pd.read_csv('../data/stimuli/political_conflict_pairs_50.csv')\n",
    "print(f\" Loaded {len(conflict_df)} political conflict items\")\n",
    "\n",
    "# Load ideological pairs (95 items)\n",
    "ideology_df = pd.read_csv('../data/stimuli/ideology_pairs.csv')\n",
    "print(f\" Loaded {len(ideology_df)} cultural-ideological items\")\n",
    "\n",
    "# Dataset summary\n",
    "total_items = len(conflict_df) + len(ideology_df)\n",
    "print(f\"\\n Dataset Summary:\")\n",
    "print(f\"   Political Conflict: {len(conflict_df)} Gaza framing pairs\")\n",
    "print(f\"   Cultural-Ideological: {len(ideology_df)} religious vs secular pairs\")\n",
    "print(f\"   Total: {total_items} stimulus pairs for evaluation\")\n",
    "\n",
    "# Preview the data\n",
    "print(\"\\n Sample political conflict item:\")\n",
    "print(f\"Context: {conflict_df.iloc[0]['context']}\")\n",
    "print(f\"Choice 1: {conflict_df.iloc[0]['choice_1']}\")\n",
    "print(f\"Choice 2: {conflict_df.iloc[0]['choice_2']}\")\n",
    "\n",
    "print(\"\\n Sample cultural-ideological item:\")\n",
    "print(f\"Context: {ideology_df.iloc[0]['context']}\")\n",
    "print(f\"Option A: {ideology_df.iloc[0]['option_a']}\")\n",
    "print(f\"Option B: {ideology_df.iloc[0]['option_b']}\")\n",
    "\n",
    "print(f\"\\nðŸ’° Total cost so far: $0.00 (everything runs locally!)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running FREE bias evaluation demo...\n",
      "ðŸ“Š Demo Analysis:\n",
      "   Political Conflict: 10 items\n",
      "   Cultural-Ideological: 10 items\n",
      "   Total demo items: 20\n",
      "\n",
      " Analyzing political conflict items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Political: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Analyzing cultural-ideological items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ideology: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Analysis complete! Generated 80 evaluations\n",
      "   Political Conflict: 40 evaluations\n",
      "   Cultural-Ideological: 40 evaluations\n",
      "ðŸ’° Total API cost: $0.00 (100% free!)\n",
      "\n",
      " Sample results by dataset:\n",
      "Political Conflict:\n",
      "   item_id          strategy  bias_score\n",
      "0        1         zero_shot   -0.858318\n",
      "1        1  chain_of_thought   -3.049852\n",
      "2        1          few_shot   -3.604851\n",
      "3        1  instruction_tune   -1.056627\n",
      "4        2         zero_shot   -5.675028\n",
      "\n",
      "Cultural-Ideological:\n",
      "    item_id          strategy  bias_score\n",
      "40        1         zero_shot    1.269813\n",
      "41        1  chain_of_thought    4.052177\n",
      "42        1          few_shot    2.955620\n",
      "43        1  instruction_tune    5.542576\n",
      "44        2         zero_shot    2.213966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  Run FREE bias evaluation on sample items (demo)\n",
    "print(\" Running FREE bias evaluation demo...\")\n",
    "\n",
    "# Initialize prompt generator\n",
    "prompt_gen = BiasPromptGenerator()\n",
    "\n",
    "# For demo, analyze first 10 items from each dataset\n",
    "demo_conflict = conflict_df.head(10)  # First 10 political conflict items\n",
    "demo_ideology = ideology_df.head(10)  # First 10 ideological items\n",
    "\n",
    "print(f\"ðŸ“Š Demo Analysis:\")\n",
    "print(f\"   Political Conflict: {len(demo_conflict)} items\")\n",
    "print(f\"   Cultural-Ideological: {len(demo_ideology)} items\")\n",
    "print(f\"   Total demo items: {len(demo_conflict) + len(demo_ideology)}\")\n",
    "\n",
    "# Process political conflict items\n",
    "conflict_results = []\n",
    "print(f\"\\n Analyzing political conflict items...\")\n",
    "\n",
    "for idx, row in tqdm(demo_conflict.iterrows(), total=len(demo_conflict), desc=\"Political\"):\n",
    "    context = row['context']\n",
    "    choices = [row['choice_1'], row['choice_2']]\n",
    "    \n",
    "    # Apply different prompting strategies (all free!)\n",
    "    strategies = ['zero_shot', 'chain_of_thought', 'few_shot', 'instruction_tune']\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        # Generate prompt using the strategy\n",
    "        prompt = prompt_gen.format_prompt(strategy, context, domain=\"political_conflict\")\n",
    "        \n",
    "        # Compute surprisal values (completely free)\n",
    "        surprisal = prober.compute_surprisal(prompt, choices)\n",
    "        bias_score = prober.compute_bias_score(surprisal)\n",
    "        \n",
    "        conflict_results.append({\n",
    "            'dataset': 'political_conflict',\n",
    "            'item_id': row['id'],\n",
    "            'strategy': strategy,\n",
    "            'context': context,\n",
    "            'choice_1': choices[0],\n",
    "            'choice_2': choices[1],\n",
    "            'surprisal_1': surprisal[0],\n",
    "            'surprisal_2': surprisal[1],\n",
    "            'bias_score': bias_score,\n",
    "            'model': 'gpt2-free'\n",
    "        })\n",
    "\n",
    "# Process ideological items\n",
    "ideology_results = []\n",
    "print(f\"\\n Analyzing cultural-ideological items...\")\n",
    "\n",
    "for idx, row in tqdm(demo_ideology.iterrows(), total=len(demo_ideology), desc=\"Ideology\"):\n",
    "    context = row['context']\n",
    "    choices = [row['option_a'], row['option_b']]  # Different column names\n",
    "    \n",
    "    # Apply different prompting strategies (all free!)\n",
    "    strategies = ['zero_shot', 'chain_of_thought', 'few_shot', 'instruction_tune']\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        # Generate prompt using the strategy\n",
    "        prompt = prompt_gen.format_prompt(strategy, context, domain=\"cultural_ideology\")\n",
    "        \n",
    "        # Compute surprisal values (completely free)\n",
    "        surprisal = prober.compute_surprisal(prompt, choices)\n",
    "        bias_score = prober.compute_bias_score(surprisal)\n",
    "        \n",
    "        ideology_results.append({\n",
    "            'dataset': 'cultural_ideology',\n",
    "            'item_id': row['pair_id'],\n",
    "            'strategy': strategy,\n",
    "            'context': context,\n",
    "            'choice_1': choices[0],\n",
    "            'choice_2': choices[1],\n",
    "            'surprisal_1': surprisal[0],\n",
    "            'surprisal_2': surprisal[1],\n",
    "            'bias_score': bias_score,\n",
    "            'model': 'gpt2-free'\n",
    "        })\n",
    "\n",
    "# Combine results\n",
    "all_results = conflict_results + ideology_results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(f\"\\n Analysis complete! Generated {len(results_df)} evaluations\")\n",
    "print(f\"   Political Conflict: {len(conflict_results)} evaluations\")\n",
    "print(f\"   Cultural-Ideological: {len(ideology_results)} evaluations\")\n",
    "print(f\"ðŸ’° Total API cost: $0.00 (100% free!)\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\n Sample results by dataset:\")\n",
    "print(\"Political Conflict:\")\n",
    "print(results_df[results_df['dataset']=='political_conflict'][['item_id', 'strategy', 'bias_score']].head())\n",
    "print(\"\\nCultural-Ideological:\")\n",
    "print(results_df[results_df['dataset']=='cultural_ideology'][['item_id', 'strategy', 'bias_score']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For complete analysis, run full dataset evaluation...\n",
      "  This will take longer but provides comprehensive results\n",
      "\n",
      " To run full analysis:\n",
      "   1. Uncomment the code above\n",
      "   2. Run the cell\n",
      "   3. Results will be saved to ../data/results/\n",
      "\n",
      "ðŸ“Š Full dataset would generate 740 total evaluations\n",
      "ðŸ’° Still 100% FREE - no API costs!\n"
     ]
    }
   ],
   "source": [
    "#  Full Dataset Analysis (Optional - Run for Complete Results)\n",
    "print(\" For complete analysis, run full dataset evaluation...\")\n",
    "print(\"  This will take longer but provides comprehensive results\")\n",
    "\n",
    "# Uncomment below to run full analysis on all 185 stimulus pairs\n",
    "\"\"\"\n",
    "# Full analysis function\n",
    "def run_full_analysis():\n",
    "    print(\"ðŸ“Š Running comprehensive bias evaluation on all datasets...\")\n",
    "    \n",
    "    all_results = []\n",
    "    strategies = ['zero_shot', 'chain_of_thought', 'few_shot', 'instruction_tune']\n",
    "    \n",
    "    # Process all political conflict items (90 pairs)\n",
    "    print(f\" Processing {len(conflict_df)} political conflict items...\")\n",
    "    for idx, row in tqdm(conflict_df.iterrows(), total=len(conflict_df), desc=\"Political Conflict\"):\n",
    "        context = row['context']\n",
    "        choices = [row['choice_1'], row['choice_2']]\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            prompt = prompt_gen.format_prompt(strategy, context, domain=\"political_conflict\")\n",
    "            surprisal = prober.compute_surprisal(prompt, choices)\n",
    "            bias_score = prober.compute_bias_score(surprisal)\n",
    "            \n",
    "            all_results.append({\n",
    "                'dataset': 'political_conflict',\n",
    "                'item_id': row['id'],\n",
    "                'strategy': strategy,\n",
    "                'context': context,\n",
    "                'choice_1': choices[0],\n",
    "                'choice_2': choices[1],\n",
    "                'surprisal_1': surprisal[0],\n",
    "                'surprisal_2': surprisal[1],\n",
    "                'bias_score': bias_score,\n",
    "                'model': 'gpt2-free'\n",
    "            })\n",
    "    \n",
    "    # Process all ideological items (95 pairs)\n",
    "    print(f\" Processing {len(ideology_df)} cultural-ideological items...\")\n",
    "    for idx, row in tqdm(ideology_df.iterrows(), total=len(ideology_df), desc=\"Cultural-Ideological\"):\n",
    "        context = row['context']\n",
    "        choices = [row['option_a'], row['option_b']]\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            prompt = prompt_gen.format_prompt(strategy, context, domain=\"cultural_ideology\")\n",
    "            surprisal = prober.compute_surprisal(prompt, choices)\n",
    "            bias_score = prober.compute_bias_score(surprisal)\n",
    "            \n",
    "            all_results.append({\n",
    "                'dataset': 'cultural_ideology',\n",
    "                'item_id': row['pair_id'],\n",
    "                'strategy': strategy,\n",
    "                'context': context,\n",
    "                'choice_1': choices[0],\n",
    "                'choice_2': choices[1],\n",
    "                'surprisal_1': surprisal[0],\n",
    "                'surprisal_2': surprisal[1],\n",
    "                'bias_score': bias_score,\n",
    "                'model': 'gpt2-free'\n",
    "            })\n",
    "    \n",
    "    # Save results\n",
    "    full_results_df = pd.DataFrame(all_results)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = f\"../data/results/full_bias_evaluation_{timestamp}.csv\"\n",
    "    \n",
    "    # Create results directory if it doesn't exist\n",
    "    os.makedirs('../data/results', exist_ok=True)\n",
    "    full_results_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\" Full analysis complete!\")\n",
    "    print(f\"   Total evaluations: {len(full_results_df)}\")\n",
    "    print(f\"   Saved to: {output_file}\")\n",
    "    \n",
    "    return full_results_df\n",
    "\n",
    "# Uncomment to run full analysis:\n",
    "# full_results = run_full_analysis()\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n To run full analysis:\")\n",
    "print(\"   1. Uncomment the code above\")\n",
    "print(\"   2. Run the cell\")\n",
    "print(\"   3. Results will be saved to ../data/results/\")\n",
    "print(f\"\\nðŸ“Š Full dataset would generate {(len(conflict_df) + len(ideology_df)) * 4} total evaluations\")\n",
    "print(\"ðŸ’° Still 100% FREE - no API costs!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogs185",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
